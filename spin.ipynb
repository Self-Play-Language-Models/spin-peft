{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install transformers datasets peft"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft.tuners.lora import LoraConfig, LoraModel"]},{"cell_type":"markdown","metadata":{},"source":["Load model and tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_checkpoint = \"gpt-neo-2.7B\"\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"markdown","metadata":{},"source":["Define lambda regularization parameter as per paper details"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lambda_reg = 0.1"]},{"cell_type":"markdown","metadata":{},"source":["Placeholder for the dataset loading function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset = [{\"prompt\": \"Example prompt\", \"response\": \"Example response\"}]"]},{"cell_type":"markdown","metadata":{},"source":["Define LoRA configuration"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lora_config = LoraConfig(\n","    r=128,  # rank of LoRA\n","    lora_alpha=256,  # scaling factor for initialization\n","    lora_dropout=0.05,\n","    bias=\"none\",\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Wrap the model with LoRA layers for parameter-efficient training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["peft_model = LoraModel(model, lora_config).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["Define the compute_spin_loss function (unchanged from previous)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_spin_loss(model_logits_gt, opponent_logits_gt, model_logits_syn, opponent_logits_syn, lambda_reg):\n","    model_probs_gt = torch.nn.functional.softmax(model_logits_gt, dim=-1)\n","    model_probs_syn = torch.nn.functional.softmax(model_logits_syn, dim=-1)\n","    opponent_probs_gt = torch.nn.functional.softmax(opponent_logits_gt, dim=-1)\n","    opponent_probs_syn = torch.nn.functional.softmax(opponent_logits_syn, dim=-1)\n","\n","    # Calculate losses\n","    loss_gt = -torch.log(model_probs_gt / opponent_probs_gt)\n","    loss_syn = -torch.log(model_probs_syn / opponent_probs_syn)\n","\n","    # Apply the logistic loss to the log odds ratio\n","    logistic_loss_gt = torch.log(1 + torch.exp(-lambda_reg * loss_gt))\n","    logistic_loss_syn = torch.log(1 + torch.exp(-lambda_reg * loss_syn))\n","\n","    # Combine losses for the final spin loss\n","    spin_loss = logistic_loss_gt.mean() + logistic_loss_syn.mean()\n","    return spin_loss"]},{"cell_type":"markdown","metadata":{},"source":["Training setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, peft_model.parameters()), lr=5e-5)"]},{"cell_type":"markdown","metadata":{},"source":["Training loop for T iterations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["T = 5  # Set the number of iterations\n","for iteration in range(T):\n","    total_loss = 0\n","    \n","    # Disable adapter layers for the opponent model\n","    peft_model.disable_adapter_layers()\n","    \n","    synthetic_data = []\n","    for data in dataset:\n","        prompt = data['prompt']\n","        # Tokenize and generate synthetic data using the opponent model\n","        prompt_ids = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n","        with torch.no_grad():\n","            peft_model.eval()  # Set model to evaluation mode\n","            synthetic_response_ids = peft_model.generate(prompt_ids, max_length=50)\n","            synthetic_data.append(synthetic_response_ids)\n","    \n","    # Enable adapter layers for training the main player model\n","    peft_model.enable_adapter_layers()\n","    \n","    # Train the main player model using the synthetic data and real responses\n","    peft_model.train()  # Set model to training mode\n","    for i, data in enumerate(dataset):\n","        ground_truth = data['response']\n","        ground_truth_ids = tokenizer(ground_truth, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n","        synthetic_response_ids = synthetic_data[i]\n","\n","        # Calculate logits for ground truth and synthetic responses\n","        main_player_logits_gt = peft_model(ground_truth_ids).logits\n","        main_player_logits_syn = peft_model(synthetic_response_ids).logits\n","\n","        # Get opponent's logits for synthetic responses (as they were generated before enabling LoRA)\n","        with torch.no_grad():\n","            opponent_logits_syn = peft_model(synthetic_response_ids).logits\n","        \n","        # Compute the loss (assuming the function is defined above)\n","        loss = compute_spin_loss(\n","            main_player_logits_gt, opponent_logits_syn, \n","            main_player_logits_syn, opponent_logits_syn, \n","            lambda_reg\n","        )\n","        total_loss += loss.item()\n","\n","        # Backpropagation and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    \n","    # Print average loss\n","    average_loss = total_loss / len(dataset)\n","    print(f\"Iteration {iteration + 1}/{T}, Average Loss: {average_loss}\")"]},{"cell_type":"markdown","metadata":{},"source":["Save the final model parameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["final_model_params = peft_model.state_dict()\n","print(\"Training complete.\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
