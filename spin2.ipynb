{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96544d1f-7614-4aad-a63a-034937d7fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft.tuners.lora import LoraConfig, LoraModel\n",
    "from peft import get_peft_model\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f6a51b-c8ac-4057-b72f-f8303b8c9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"TinyLlama/TinyLlama-1.1B-step-50K-105b\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, device_map=\"auto\", load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eaf3441-d8ee-44ac-8319-19685983beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reg = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3850671c-ad8b-4f14-a54a-6ac61f6db310",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft[:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b91c0bf3-bd17-4cee-be4e-a148991282c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "tstr = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8851b881-a24e-42e9-87ca-c5114f379ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(item):\n",
    "    \n",
    "    #conv = get_conversation_template(\"vicuna\")\n",
    "    #roles = {\"human\": 'user', \"gpt\": 'assistant'}\n",
    "\n",
    "    add_generation_prompt = True  # You can set this according to your needs\n",
    "\n",
    "    output = template.render(messages=item[\"messages\"], add_generation_prompt=add_generation_prompt, eos_token='\\n')\n",
    "    \n",
    "    return {\"prompt\": item[\"prompt\"], \"response\": output}\n",
    "template = Template(tstr)\n",
    "ds = ds.map(preprocess, remove_columns=[\"messages\", \"prompt_id\"], batched=False)\n",
    "dataset = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37a8a4d6-6103-4cec-9947-21ccab1355e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,  # rank of LoRA\n",
    "    lora_alpha=64,  # scaling factor for initialization\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config) #LoraModel(model, lora_config, adapter_name=\"iter0\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47818a13-baac-4891-9855-30ba1c9c0622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spin_loss(model_logits_gt, opponent_logits_gt, model_logits_syn, opponent_logits_syn, lambda_reg):\n",
    "    model_probs_gt = torch.nn.functional.softmax(model_logits_gt, dim=-1)\n",
    "    model_probs_syn = torch.nn.functional.softmax(model_logits_syn, dim=-1)\n",
    "    opponent_probs_gt = torch.nn.functional.softmax(opponent_logits_gt, dim=-1)\n",
    "    opponent_probs_syn = torch.nn.functional.softmax(opponent_logits_syn, dim=-1)\n",
    "\n",
    "    print(model_probs_gt.shape)\n",
    "\n",
    "    if model_probs_gt.shape[1] < model_probs_syn.shape[1]:\n",
    "        model_probs_syn = model_probs_syn[:, :model_probs_gt.shape[1]]\n",
    "\n",
    "    if model_probs_gt.shape[1] > model_probs_syn.shape[1]:\n",
    "        model_probs_gt = model_probs_gt[:, :model_probs_syn.shape[1]]\n",
    "    if opponent_probs_gt.shape[1] < opponent_probs_syn.shape[1]:\n",
    "        model_probs_syn = model_probs_syn[:, :model_probs_gt.shape[1]]\n",
    "\n",
    "    if opponent_probs_gt.shape[1] > opponent_probs_syn.shape[1]:\n",
    "        model_probs_gt = model_probs_gt[:, :model_probs_syn.shape[1]]\n",
    "\n",
    "    # Calculate losses\n",
    "    loss_gt = -torch.log(model_probs_gt / opponent_probs_gt)\n",
    "    loss_syn = -torch.log(model_probs_syn / opponent_probs_syn)\n",
    "\n",
    "    # Apply the logistic loss to the log odds ratio\n",
    "    logistic_loss_gt = torch.log(1 + torch.exp(-lambda_reg * loss_gt))\n",
    "    logistic_loss_syn = torch.log(1 + torch.exp(-lambda_reg * loss_syn))\n",
    "\n",
    "    # Combine losses for the final spin loss\n",
    "    spin_loss = logistic_loss_gt.mean(dim=[1,2]) + logistic_loss_syn.mean(dim=[1,2])\n",
    "    return spin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b4a28-4906-4aa2-b33c-24bcca4cc768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "226ed473-3201-40d0-89cf-078453844fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'custom'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'DataCollatorForSeq2Seq' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 93\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#print(dataset.columns)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#print(encoded_dataset[0])\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#dataset = dataset.remove_columns(dataset[\"train\"].column_names)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#print(encoded_dataset)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SPINTrainer(\n\u001b[1;32m     85\u001b[0m     model\u001b[38;5;241m=\u001b[39mpeft_model,\n\u001b[1;32m     86\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mencoded_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForSeq2Seq\n\u001b[1;32m     90\u001b[0m )\n\u001b[0;32m---> 93\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2765\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2748\u001b[0m \u001b[38;5;124;03mPerform a training step on a batch of inputs.\u001b[39;00m\n\u001b[1;32m   2749\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2762\u001b[0m \u001b[38;5;124;03m    `torch.Tensor`: The tensor with training loss on this batch.\u001b[39;00m\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2764\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m-> 2765\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2768\u001b[0m     loss_mb \u001b[38;5;241m=\u001b[39m smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2718\u001b[0m, in \u001b[0;36mTrainer._prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2714\u001b[0m \u001b[38;5;124;03mPrepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\u001b[39;00m\n\u001b[1;32m   2715\u001b[0m \u001b[38;5;124;03mhandling potential state.\u001b[39;00m\n\u001b[1;32m   2716\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2717\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs)\n\u001b[0;32m-> 2718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2720\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch received was empty, your model won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be able to train on it. Double-check that your \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2721\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining dataset contains keys expected by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2722\u001b[0m     )\n\u001b[1;32m   2723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'DataCollatorForSeq2Seq' has no len()"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "T = 5  # Set the number of iterations\n",
    "total_loss = 0\n",
    "\n",
    "# Disable adapter layers for the opponent model\n",
    "peft_model.disable_adapter_layers()\n",
    "\n",
    "synthetic_data = []\n",
    "#for data in tqdm(dataset):\n",
    "#    prompt = data['prompt']\n",
    "#    # Tokenize and generate synthetic data using the opponent model\n",
    "#    prompt_ids = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "#    with torch.no_grad():\n",
    "#       peft_model.eval()  # Set model to evaluation mode\n",
    "#        synthetic_response_ids = peft_model.generate(prompt_ids, max_length=50)\n",
    "#        synthetic_data.append(synthetic_response_ids)\n",
    "\n",
    "# Enable adapter layers for training the main player model\n",
    "peft_model.enable_adapter_layers()\n",
    "\n",
    "# Train the main player model using the synthetic data and real responses\n",
    "peft_model.train()  # Set model to training mode\n",
    "\n",
    "class SPINTrainer(Trainer):\n",
    "    def compute_loss(self, peft_model, inputs, return_outputs=False):\n",
    "        print(inputs)\n",
    "\n",
    "        exit(0)\n",
    "\n",
    "        #prompt_ids = inputs[\"input_ids\"]\n",
    "        prompt = inputs['query']\n",
    "        \n",
    "        # Tokenize and generate synthetic data using the opponent model\n",
    "        prompt_ids = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            peft_model.eval()  # Set model to evaluation mode\n",
    "            synthetic_response_ids = peft_model.generate(prompt_ids, max_length=50)\n",
    "            synthetic_data.append(synthetic_response_ids)\n",
    "\n",
    "            ground_truth = data['response']\n",
    "            ground_truth_ids = tokenizer(ground_truth, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "            synthetic_response_ids = synthetic_data[i]\n",
    "    \n",
    "            # Calculate logits for ground truth and synthetic responses\n",
    "            main_player_logits_gt = peft_model(ground_truth_ids).logits\n",
    "            main_player_logits_syn = peft_model(synthetic_response_ids).logits\n",
    "    \n",
    "            # Get opponent's logits for synthetic responses (as they were generated before enabling LoRA)\n",
    "            opponent_logits_syn = peft_model(synthetic_response_ids).logits\n",
    "            \n",
    "            # Compute the loss (assuming the function is defined above)\n",
    "            loss = compute_spin_loss(\n",
    "                main_player_logits_gt, opponent_logits_syn, \n",
    "                main_player_logits_syn, opponent_logits_syn, \n",
    "                lambda_reg\n",
    "            )\n",
    "    \n",
    "        return (loss, synthetic_response_ids) if return_outputs else loss\n",
    "\n",
    "    \n",
    "args = TrainingArguments(remove_unused_columns=False, output_dir=\"dir\", label_names=\"labels\")\n",
    "\n",
    "def tokenize_func(examples):\n",
    "    ret =  tokenizer(examples[\"prompt\"], padding=True, truncation=True, max_length=512)  # max_length=512,  padding=True\n",
    "    ret[\"custom\"] =  tokenizer(examples[\"response\"], padding=True, truncation=True, max_length=512)\n",
    "    return ret\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize_func, remove_columns=[\"prompt\", \"response\"])\n",
    "print(encoded_dataset)\n",
    "#print(dataset.columns)\n",
    "#print(encoded_dataset[0])\n",
    "#dataset = dataset.remove_columns(dataset[\"train\"].column_names)\n",
    "\n",
    "#print(encoded_dataset)\n",
    "\n",
    "trainer = SPINTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=encoded_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args = args,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd05ffd-fcbe-47eb-9665-914a66eabc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model parameters\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "final_model_params = peft_model.state_dict()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cfad2-5fa5-4068-90bc-9325ef263803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ea1e1-234e-4050-b8ef-47abcd4c0d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4df52c-76a0-4062-b222-f78664b20701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
